{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk3oEy9Z1nGm"
      },
      "source": [
        "# Language Model: Text Generation in the Style of Agatha Christie\n",
        "\n",
        "---\n",
        "\n",
        "## How to Run This Notebook\n",
        "\n",
        "1. **Enable GPU Runtime**  \n",
        "   Go to **Runtime -> Change runtime type** and select a higher-memory GPU such as T4 or L4 for optimal performance during text generation.\n",
        "\n",
        "2. **Execute the Cells in Order**  \n",
        "Run the notebook cells sequentially:\n",
        "- Import the required Python libraries. (Section 1)\n",
        "- Mount your Google Drive to access the saved files. (Section 1)\n",
        "- Load the trained model weights and the vocabulary mappings. (Section 7)\n",
        "- Load or define the `TextGenerator` class that handles text generation. (Section 5.1)  \n",
        "- Test the model on the code cell in Section 9\n",
        "\n",
        "3. **Required Files**  \n",
        "Make sure the following files are available in your Google Drive (or the working directory):  \n",
        "- `Language_Model_Hoang_Nguyen_Lai.ipynb`: the complete training and testing notebook  \n",
        "- `language model.keras`: the saved trained model weights  \n",
        "- `vocab.json`: vocabulary file with token-to-ID and ID-to-token mappings  \n",
        "- `61262-0.txt`: the original source text used to train the model\n",
        "\n",
        "4. **Interactive Demonstration**  \n",
        "After loading everything successfully, the last cell launches an interactive prompt.  \n",
        "- Enter any starting text: a single word, a sentence, or even a full paragraph.  \n",
        "- The model will continue the input in Agatha Christie’s distinctive style, generating **at least 20 additional words**.  \n",
        "- To quit the interactive loop, simply type `q` and press Enter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q4sir9QmlkC"
      },
      "source": [
        "## 1 Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xqRK-xX-WWuT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uu3-EVcy0S7D",
        "outputId": "c496e725-3745-406a-cb26-e3818c99e077"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.19.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Bp19B0PPYn",
        "outputId": "278c8ffe-1e05-4806-a531-bbd57fe04e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT0URoqpGErn",
        "outputId": "f532439f-003f-4dab-c221-7a2970739c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 296991 characters\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/61262-0.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "print(f'Length of text: {len(raw_text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplT4J_EQf_8",
        "outputId": "98d1d99e-0fe9-48de-fb47-3fb69f930b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK 61262 ***\n",
            "\n",
            "  POIROT INVESTIGATES\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  BY THE SAME AUTHOR\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(raw_text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiQE66fMrUUd",
        "outputId": "34dd5494-8d78-4b8a-966f-17928011de59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " EBook of Poirot Investigates, by Agatha Christie\n",
            "\n",
            "*** END OF THE PROJECT GUTENBERG EBOOK 61262 ***\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(raw_text[-100:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Qu-mtumsRA"
      },
      "source": [
        "## 2 Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1DK7BUq1nGt"
      },
      "source": [
        "### 2.1 Cleaning the data (text file)\n",
        "\n",
        "Remove Gutenberg Boilerplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DvwNCeDQiSz",
        "outputId": "0c64d591-65ab-4abb-9ea0-8cb8db4d305e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned length: 296566 characters\n",
            "POIROT INVESTIGATES\n",
            "\n",
            " BY THE SAME AUTHOR\n",
            "\n",
            " THE MYSTERIOUS AFFAIR AT STYLES\n",
            "\n",
            " THE SECRET ADVERSARY\n",
            "\n",
            " THE MURDER ON THE LINKS\n",
            "\n",
            " THE BODLEY HEAD\n",
            "\n",
            " POIROT INVESTIGATES\n",
            "\n",
            " BY AGATHA CHRISTIE\n",
            "\n",
            " LONDON\n",
            "\n",
            " JOHN LANE THE BODLEY HEAD LIMITED\n",
            "\n",
            " First published in Great Britain by\n",
            " John Lane Company, The Bodley Head Limited, 1924\n",
            "\n",
            " Copyright © 1924 Agatha Christie Limited\n",
            "\n",
            " CONTENTS\n",
            "\n",
            " I The Adventure of “The Western Star”\n",
            "\n",
            " II The Tragedy at Marsdon Manor\n",
            "\n",
            " III The Adventure of the Cheap Flat\n",
            "\n",
            " IV The Mystery\n"
          ]
        }
      ],
      "source": [
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "start_idx = raw_text.find(start_marker)\n",
        "if start_idx != -1:\n",
        "    start_idx = raw_text.index('\\n', start_idx) + 1  # skip the marker line\n",
        "else:\n",
        "    start_idx = 0\n",
        "\n",
        "end_idx = raw_text.find(end_marker)\n",
        "if end_idx == -1:\n",
        "    end_idx = len(raw_text)\n",
        "\n",
        "text = raw_text[start_idx:end_idx].strip()\n",
        "\n",
        "# Light cleaning: normalize whitespace, keep punctuation\n",
        "text = re.sub(r'\\n{3,}', '\\n\\n', text)       # collapse excessive newlines\n",
        "text = re.sub(r'[ \\t]+', ' ', text)           # collapse spaces/tabs\n",
        "text = text.replace('\\r', '')\n",
        "\n",
        "print(f'Cleaned length: {len(text)} characters')\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq3ga5cKI0te"
      },
      "source": [
        "### 2.2 Word-Level Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1HaE01vtna1",
        "outputId": "0909e381-cfa6-431a-d8aa-ff4d472b447b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 68373\n",
            "First 50 tokens: ['POIROT', 'INVESTIGATES', 'BY', 'THE', 'SAME', 'AUTHOR', 'THE', 'MYSTERIOUS', 'AFFAIR', 'AT', 'STYLES', 'THE', 'SECRET', 'ADVERSARY', 'THE', 'MURDER', 'ON', 'THE', 'LINKS', 'THE', 'BODLEY', 'HEAD', 'POIROT', 'INVESTIGATES', 'BY', 'AGATHA', 'CHRISTIE', 'LONDON', 'JOHN', 'LANE', 'THE', 'BODLEY', 'HEAD', 'LIMITED', 'First', 'published', 'in', 'Great', 'Britain', 'by', 'John', 'Lane', 'Company', ',', 'The', 'Bodley', 'Head', 'Limited', ',', '1924']\n",
            "Unique tokens: 6501\n",
            "Vocabulary size (min_freq=2): 3295\n",
            "Encoded corpus shape: (68373,)\n",
            "Sample decode: POIROT INVESTIGATES BY THE <UNK> <UNK> THE <UNK> <UNK> <UNK> <UNK> THE <UNK> <UNK> THE <UNK> <UNK> THE <UNK> THE\n"
          ]
        }
      ],
      "source": [
        "# Split text into words while keeping punctuation as separate tokens\n",
        "def tokenize(text):\n",
        "    \"\"\"Split into words + punctuation tokens.\"\"\"\n",
        "    # This regex splits on whitespace but keeps punctuation as separate tokens\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "tokens = tokenize(text)\n",
        "print(f'Total tokens: {len(tokens)}')\n",
        "print(f'First 50 tokens: {tokens[:50]}')\n",
        "\n",
        "# Build vocabulary\n",
        "from collections import Counter\n",
        "token_counts = Counter(tokens)\n",
        "print(f'Unique tokens: {len(token_counts)}')\n",
        "\n",
        "# Keep tokens that appear at least 2 times (reduces noise)\n",
        "MIN_FREQ = 2\n",
        "vocab_tokens = ['<UNK>'] + [t for t, c in token_counts.most_common() if c >= MIN_FREQ]\n",
        "vocab_size = len(vocab_tokens)\n",
        "print(f'Vocabulary size (min_freq={MIN_FREQ}): {vocab_size}')\n",
        "\n",
        "# Create mappings\n",
        "token_to_id = {t: i for i, t in enumerate(vocab_tokens)}\n",
        "id_to_token = {i: t for t, i in token_to_id.items()}\n",
        "UNK_ID = 0\n",
        "\n",
        "def encode(token_list):\n",
        "    return [token_to_id.get(t, UNK_ID) for t in token_list]\n",
        "\n",
        "def decode(id_list):\n",
        "    return ' '.join([id_to_token.get(i, '<UNK>') for i in id_list])\n",
        "\n",
        "# Encode entire corpus\n",
        "all_ids = encode(tokens)\n",
        "all_ids = np.array(all_ids, dtype=np.int32)\n",
        "print(f'Encoded corpus shape: {all_ids.shape}')\n",
        "print(f'Sample decode: {decode(all_ids[:20])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYTdvQW1nGt"
      },
      "source": [
        "**Why word-level over character-level?**\n",
        "\n",
        "Character-level models (vocab ~80) train easily but produce misspelled words and lack semantic structure. Word-level models predict actual words, producing more coherent output at the cost of a larger vocabulary.\n",
        "\n",
        "**Vocabulary construction:**\n",
        "\n",
        "- Regex tokeniser splits text into words and punctuation: `\\w+|[^\\w\\s]`\n",
        "- Tokens appearing < `MIN_FREQ` times are mapped to `<UNK>`\n",
        "- `MIN_FREQ=2` was chosen after comparing thresholds (see table below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqJcaDe_JEQJ"
      },
      "source": [
        "### 2.3 Create Training Sequences\n",
        "\n",
        "The encoded token sequence is split into overlapping windows of `SEQ_LENGTH + 1` tokens. Each window produces an (input, target) pair where the target is the input shifted by one position. This teaches the model to predict the next word given the preceding context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtqAPmpYtwr4",
        "outputId": "6642d804-6438-4dfa-ca58-0b0b1078fccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : POIROT INVESTIGATES BY THE <UNK> <UNK> THE <UNK> <UNK> <UNK> <UNK> THE <UNK> <UNK> THE <UNK> <UNK> THE <UNK> THE BODLEY HEAD POIROT INVESTIGATES BY <UNK> <UNK> <UNK> <UNK> <UNK> THE BODLEY HEAD <UNK> First published in Great Britain by John <UNK> Company , The <UNK> <UNK> Limited , 1924\n",
            "Target: INVESTIGATES BY THE <UNK> <UNK> THE <UNK> <UNK> <UNK> <UNK> THE <UNK> <UNK> THE <UNK> <UNK> THE <UNK> THE BODLEY HEAD POIROT INVESTIGATES BY <UNK> <UNK> <UNK> <UNK> <UNK> THE BODLEY HEAD <UNK> First published in Great Britain by John <UNK> Company , The <UNK> <UNK> Limited , 1924 <UNK>\n"
          ]
        }
      ],
      "source": [
        "SEQ_LENGTH = 50\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Verify\n",
        "for inp, tgt in dataset.take(1):\n",
        "    print(\"Input :\", decode(inp.numpy()))\n",
        "    print(\"Target:\", decode(tgt.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIPcxeQ5i6l-",
        "outputId": "7e579d6c-f4a6-4594-b916-7c68ab64db8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 50), dtype=tf.int32, name=None), TensorSpec(shape=(64, 50), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NviCwKrUJYvF"
      },
      "source": [
        "## 3 Model Architecture: Model design and Hyperparameter Selection\n",
        "\n",
        "- **Embedding layer (256-dim):** Maps each vocabulary token to a dense vector representation\n",
        "- **2× GRU layers (512 units each):** Stacked recurrent layers capture hierarchical sequential patterns. The first layer learns word-level transitions; the second learns phrase-level structure. GRU was chosen over LSTM for faster training with comparable performance on this corpus size.\n",
        "- **Dropout (0.2):** Applied within GRU layers to reduce overfitting on the small (~68K token) corpus\n",
        "- **Dense output layer:** Projects to vocabulary size with logits for next-token prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "uvh6tPVd1_RJ",
        "outputId": "c2f58072-e6f8-4cf5-8817-36964fcbfb6e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │       \u001b[38;5;34m843,520\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3295\u001b[0m)         │     \u001b[38;5;34m1,690,335\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">843,520</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3295</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,690,335</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,292,511\u001b[0m (20.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,292,511</span> (20.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,292,511\u001b[0m (20.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,292,511</span> (20.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 50, 3295) # (batch_size, seq_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 512\n",
        "DROPOUT = 0.2\n",
        "NUM_LAYERS = 2\n",
        "\n",
        "# Build stacked GRU model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, EMBEDDING_DIM),\n",
        "    tf.keras.layers.GRU(RNN_UNITS, return_sequences=True, dropout=DROPOUT),\n",
        "    tf.keras.layers.GRU(RNN_UNITS, return_sequences=True, dropout=DROPOUT),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "model.build(input_shape=(BATCH_SIZE, SEQ_LENGTH))\n",
        "model.summary()\n",
        "\n",
        "# Verify output shape\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, seq_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRzHV2s8JU8N"
      },
      "source": [
        "## 4 Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lccmm8B1nGu"
      },
      "source": [
        "### 4.1 Training Strategy\n",
        "\n",
        "- **Optimiser:** Adam with initial learning rate 2e-3\n",
        "- **Loss:** Sparse categorical cross-entropy (from logits)\n",
        "- **LR Schedule:** ReduceLROnPlateau (factor=0.5, patience=5)\n",
        "- **Early Stopping:** patience=15, restores best weights\n",
        "- **Checkpointing:** Saves weights every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAcPlGs52J7m",
        "outputId": "e28b0307-aab0-4da7-fcfb-3ae2b3e84490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - accuracy: 0.0466 - loss: 7.0643 - learning_rate: 0.0020\n",
            "Epoch 2/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0589 - loss: 5.9858 - learning_rate: 0.0020\n",
            "Epoch 3/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0600 - loss: 5.9335 - learning_rate: 0.0020\n",
            "Epoch 4/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0661 - loss: 5.9021 - learning_rate: 0.0020\n",
            "Epoch 5/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0682 - loss: 5.8776 - learning_rate: 0.0020\n",
            "Epoch 6/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0767 - loss: 5.8127 - learning_rate: 0.0020\n",
            "Epoch 7/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0843 - loss: 5.7350 - learning_rate: 0.0020\n",
            "Epoch 8/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0932 - loss: 5.6613 - learning_rate: 0.0020\n",
            "Epoch 9/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1109 - loss: 5.5634 - learning_rate: 0.0020\n",
            "Epoch 10/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1135 - loss: 5.5176 - learning_rate: 0.0020\n",
            "Epoch 11/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1128 - loss: 5.4439 - learning_rate: 0.0020\n",
            "Epoch 12/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1168 - loss: 5.3933 - learning_rate: 0.0020\n",
            "Epoch 13/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1202 - loss: 5.3428 - learning_rate: 0.0020\n",
            "Epoch 14/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1247 - loss: 5.2925 - learning_rate: 0.0020\n",
            "Epoch 15/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1197 - loss: 5.2678 - learning_rate: 0.0020\n",
            "Epoch 16/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1254 - loss: 5.2323 - learning_rate: 0.0020\n",
            "Epoch 17/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1280 - loss: 5.1810 - learning_rate: 0.0020\n",
            "Epoch 18/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1259 - loss: 5.1563 - learning_rate: 0.0020\n",
            "Epoch 19/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1322 - loss: 5.1310 - learning_rate: 0.0020\n",
            "Epoch 20/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1327 - loss: 5.1049 - learning_rate: 0.0020\n",
            "Epoch 21/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1368 - loss: 5.0606 - learning_rate: 0.0020\n",
            "Epoch 22/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1414 - loss: 5.0317 - learning_rate: 0.0020\n",
            "Epoch 23/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1414 - loss: 4.9936 - learning_rate: 0.0020\n",
            "Epoch 24/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1451 - loss: 4.9483 - learning_rate: 0.0020\n",
            "Epoch 25/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1487 - loss: 4.8999 - learning_rate: 0.0020\n",
            "Epoch 26/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1559 - loss: 4.8369 - learning_rate: 0.0020\n",
            "Epoch 27/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1590 - loss: 4.7861 - learning_rate: 0.0020\n",
            "Epoch 28/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1635 - loss: 4.7140 - learning_rate: 0.0020\n",
            "Epoch 29/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1698 - loss: 4.6790 - learning_rate: 0.0020\n",
            "Epoch 30/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1725 - loss: 4.6341 - learning_rate: 0.0020\n",
            "Epoch 31/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1804 - loss: 4.5872 - learning_rate: 0.0020\n",
            "Epoch 32/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1832 - loss: 4.5569 - learning_rate: 0.0020\n",
            "Epoch 33/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1871 - loss: 4.4989 - learning_rate: 0.0020\n",
            "Epoch 34/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1911 - loss: 4.4487 - learning_rate: 0.0020\n",
            "Epoch 35/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1976 - loss: 4.3987 - learning_rate: 0.0020\n",
            "Epoch 36/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2038 - loss: 4.3419 - learning_rate: 0.0020\n",
            "Epoch 37/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2053 - loss: 4.2915 - learning_rate: 0.0020\n",
            "Epoch 38/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2126 - loss: 4.2341 - learning_rate: 0.0020\n",
            "Epoch 39/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2171 - loss: 4.1822 - learning_rate: 0.0020\n",
            "Epoch 40/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2220 - loss: 4.1418 - learning_rate: 0.0020\n",
            "Epoch 41/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2260 - loss: 4.0955 - learning_rate: 0.0020\n",
            "Epoch 42/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2309 - loss: 4.0211 - learning_rate: 0.0020\n",
            "Epoch 43/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2346 - loss: 3.9778 - learning_rate: 0.0020\n",
            "Epoch 44/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2411 - loss: 3.9356 - learning_rate: 0.0020\n",
            "Epoch 45/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2446 - loss: 3.8900 - learning_rate: 0.0020\n",
            "Epoch 46/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2493 - loss: 3.8246 - learning_rate: 0.0020\n",
            "Epoch 47/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2535 - loss: 3.7908 - learning_rate: 0.0020\n",
            "Epoch 48/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2623 - loss: 3.7177 - learning_rate: 0.0020\n",
            "Epoch 49/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2651 - loss: 3.6556 - learning_rate: 0.0020\n",
            "Epoch 50/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2684 - loss: 3.6311 - learning_rate: 0.0020\n",
            "Epoch 51/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2755 - loss: 3.5794 - learning_rate: 0.0020\n",
            "Epoch 52/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2823 - loss: 3.5178 - learning_rate: 0.0020\n",
            "Epoch 53/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2864 - loss: 3.4690 - learning_rate: 0.0020\n",
            "Epoch 54/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.2927 - loss: 3.4009 - learning_rate: 0.0020\n",
            "Epoch 55/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3052 - loss: 3.3314 - learning_rate: 0.0020\n",
            "Epoch 56/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3024 - loss: 3.3314 - learning_rate: 0.0020\n",
            "Epoch 57/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3104 - loss: 3.2666 - learning_rate: 0.0020\n",
            "Epoch 58/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3162 - loss: 3.2074 - learning_rate: 0.0020\n",
            "Epoch 59/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3225 - loss: 3.1803 - learning_rate: 0.0020\n",
            "Epoch 60/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3248 - loss: 3.1431 - learning_rate: 0.0020\n",
            "Epoch 61/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3375 - loss: 3.0586 - learning_rate: 0.0020\n",
            "Epoch 62/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3495 - loss: 2.9864 - learning_rate: 0.0020\n",
            "Epoch 63/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3531 - loss: 2.9524 - learning_rate: 0.0020\n",
            "Epoch 64/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3582 - loss: 2.9134 - learning_rate: 0.0020\n",
            "Epoch 65/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3628 - loss: 2.8669 - learning_rate: 0.0020\n",
            "Epoch 66/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3697 - loss: 2.8227 - learning_rate: 0.0020\n",
            "Epoch 67/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3797 - loss: 2.7651 - learning_rate: 0.0020\n",
            "Epoch 68/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3924 - loss: 2.7061 - learning_rate: 0.0020\n",
            "Epoch 69/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.3982 - loss: 2.6516 - learning_rate: 0.0020\n",
            "Epoch 70/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4026 - loss: 2.6039 - learning_rate: 0.0020\n",
            "Epoch 71/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4129 - loss: 2.5644 - learning_rate: 0.0020\n",
            "Epoch 72/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4262 - loss: 2.4949 - learning_rate: 0.0020\n",
            "Epoch 73/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4312 - loss: 2.4516 - learning_rate: 0.0020\n",
            "Epoch 74/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4471 - loss: 2.3895 - learning_rate: 0.0020\n",
            "Epoch 75/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4517 - loss: 2.3619 - learning_rate: 0.0020\n",
            "Epoch 76/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4658 - loss: 2.2845 - learning_rate: 0.0020\n",
            "Epoch 77/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4693 - loss: 2.2668 - learning_rate: 0.0020\n",
            "Epoch 78/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.4816 - loss: 2.2169 - learning_rate: 0.0020\n",
            "Epoch 79/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4894 - loss: 2.1736 - learning_rate: 0.0020\n",
            "Epoch 80/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5021 - loss: 2.1146 - learning_rate: 0.0020\n",
            "Epoch 81/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5102 - loss: 2.0621 - learning_rate: 0.0020\n",
            "Epoch 82/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5201 - loss: 2.0211 - learning_rate: 0.0020\n",
            "Epoch 83/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5327 - loss: 1.9556 - learning_rate: 0.0020\n",
            "Epoch 84/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5454 - loss: 1.9113 - learning_rate: 0.0020\n",
            "Epoch 85/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5489 - loss: 1.8880 - learning_rate: 0.0020\n",
            "Epoch 86/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5647 - loss: 1.8249 - learning_rate: 0.0020\n",
            "Epoch 87/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5708 - loss: 1.7879 - learning_rate: 0.0020\n",
            "Epoch 88/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5829 - loss: 1.7354 - learning_rate: 0.0020\n",
            "Epoch 89/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5918 - loss: 1.7131 - learning_rate: 0.0020\n",
            "Epoch 90/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.5980 - loss: 1.6690 - learning_rate: 0.0020\n",
            "Epoch 91/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6043 - loss: 1.6380 - learning_rate: 0.0020\n",
            "Epoch 92/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6157 - loss: 1.6027 - learning_rate: 0.0020\n",
            "Epoch 93/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6237 - loss: 1.5624 - learning_rate: 0.0020\n",
            "Epoch 94/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6364 - loss: 1.5078 - learning_rate: 0.0020\n",
            "Epoch 95/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.6448 - loss: 1.4682 - learning_rate: 0.0020\n",
            "Epoch 96/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6479 - loss: 1.4453 - learning_rate: 0.0020\n",
            "Epoch 97/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6573 - loss: 1.4144 - learning_rate: 0.0020\n",
            "Epoch 98/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6643 - loss: 1.3952 - learning_rate: 0.0020\n",
            "Epoch 99/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6769 - loss: 1.3397 - learning_rate: 0.0020\n",
            "Epoch 100/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6824 - loss: 1.3004 - learning_rate: 0.0020\n",
            "Epoch 101/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6942 - loss: 1.2640 - learning_rate: 0.0020\n",
            "Epoch 102/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6951 - loss: 1.2498 - learning_rate: 0.0020\n",
            "Epoch 103/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7032 - loss: 1.2183 - learning_rate: 0.0020\n",
            "Epoch 104/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7050 - loss: 1.2106 - learning_rate: 0.0020\n",
            "Epoch 105/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7234 - loss: 1.1481 - learning_rate: 0.0020\n",
            "Epoch 106/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7219 - loss: 1.1376 - learning_rate: 0.0020\n",
            "Epoch 107/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7298 - loss: 1.1189 - learning_rate: 0.0020\n",
            "Epoch 108/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7385 - loss: 1.0759 - learning_rate: 0.0020\n",
            "Epoch 109/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7457 - loss: 1.0482 - learning_rate: 0.0020\n",
            "Epoch 110/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7542 - loss: 1.0141 - learning_rate: 0.0020\n",
            "Epoch 111/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7573 - loss: 1.0092 - learning_rate: 0.0020\n",
            "Epoch 112/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7647 - loss: 0.9582 - learning_rate: 0.0020\n",
            "Epoch 113/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7670 - loss: 0.9610 - learning_rate: 0.0020\n",
            "Epoch 114/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7739 - loss: 0.9319 - learning_rate: 0.0020\n",
            "Epoch 115/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7784 - loss: 0.9082 - learning_rate: 0.0020\n",
            "Epoch 116/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.7869 - loss: 0.8854 - learning_rate: 0.0020\n",
            "Epoch 117/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7873 - loss: 0.8744 - learning_rate: 0.0020\n",
            "Epoch 118/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7955 - loss: 0.8478 - learning_rate: 0.0020\n",
            "Epoch 119/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7991 - loss: 0.8335 - learning_rate: 0.0020\n",
            "Epoch 120/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8027 - loss: 0.8110 - learning_rate: 0.0020\n",
            "Epoch 121/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8132 - loss: 0.7772 - learning_rate: 0.0020\n",
            "Epoch 122/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8143 - loss: 0.7732 - learning_rate: 0.0020\n",
            "Epoch 123/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8157 - loss: 0.7586 - learning_rate: 0.0020\n",
            "Epoch 124/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8197 - loss: 0.7414 - learning_rate: 0.0020\n",
            "Epoch 125/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8263 - loss: 0.7274 - learning_rate: 0.0020\n",
            "Epoch 126/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8281 - loss: 0.7074 - learning_rate: 0.0020\n",
            "Epoch 127/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8328 - loss: 0.6943 - learning_rate: 0.0020\n",
            "Epoch 128/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8367 - loss: 0.6717 - learning_rate: 0.0020\n",
            "Epoch 129/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8417 - loss: 0.6582 - learning_rate: 0.0020\n",
            "Epoch 130/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8431 - loss: 0.6500 - learning_rate: 0.0020\n",
            "Epoch 131/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8483 - loss: 0.6340 - learning_rate: 0.0020\n",
            "Epoch 132/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8518 - loss: 0.6226 - learning_rate: 0.0020\n",
            "Epoch 133/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8540 - loss: 0.6076 - learning_rate: 0.0020\n",
            "Epoch 134/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8582 - loss: 0.5933 - learning_rate: 0.0020\n",
            "Epoch 135/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8573 - loss: 0.5852 - learning_rate: 0.0020\n",
            "Epoch 136/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8639 - loss: 0.5709 - learning_rate: 0.0020\n",
            "Epoch 137/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8654 - loss: 0.5636 - learning_rate: 0.0020\n",
            "Epoch 138/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8677 - loss: 0.5502 - learning_rate: 0.0020\n",
            "Epoch 139/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8753 - loss: 0.5266 - learning_rate: 0.0020\n",
            "Epoch 140/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8798 - loss: 0.5121 - learning_rate: 0.0020\n",
            "Epoch 141/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8801 - loss: 0.5096 - learning_rate: 0.0020\n",
            "Epoch 142/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8829 - loss: 0.4960 - learning_rate: 0.0020\n",
            "Epoch 143/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8797 - loss: 0.5021 - learning_rate: 0.0020\n",
            "Epoch 144/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8841 - loss: 0.4904 - learning_rate: 0.0020\n",
            "Epoch 145/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8863 - loss: 0.4805 - learning_rate: 0.0020\n",
            "Epoch 146/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8879 - loss: 0.4633 - learning_rate: 0.0020\n",
            "Epoch 147/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8929 - loss: 0.4570 - learning_rate: 0.0020\n",
            "Epoch 148/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8903 - loss: 0.4612 - learning_rate: 0.0020\n",
            "Epoch 149/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8954 - loss: 0.4414 - learning_rate: 0.0020\n",
            "Epoch 150/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8950 - loss: 0.4419 - learning_rate: 0.0020\n",
            "Epoch 151/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8960 - loss: 0.4343 - learning_rate: 0.0020\n",
            "Epoch 152/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8989 - loss: 0.4220 - learning_rate: 0.0020\n",
            "Epoch 153/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9009 - loss: 0.4165 - learning_rate: 0.0020\n",
            "Epoch 154/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9012 - loss: 0.4097 - learning_rate: 0.0020\n",
            "Epoch 155/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9044 - loss: 0.4046 - learning_rate: 0.0020\n",
            "Epoch 156/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9059 - loss: 0.3966 - learning_rate: 0.0020\n",
            "Epoch 157/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9057 - loss: 0.3945 - learning_rate: 0.0020\n",
            "Epoch 158/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9059 - loss: 0.3887 - learning_rate: 0.0020\n",
            "Epoch 159/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9093 - loss: 0.3786 - learning_rate: 0.0020\n",
            "Epoch 160/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9121 - loss: 0.3703 - learning_rate: 0.0020\n",
            "Epoch 161/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9114 - loss: 0.3729 - learning_rate: 0.0020\n",
            "Epoch 162/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9141 - loss: 0.3597 - learning_rate: 0.0020\n",
            "Epoch 163/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9162 - loss: 0.3583 - learning_rate: 0.0020\n",
            "Epoch 164/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9171 - loss: 0.3512 - learning_rate: 0.0020\n",
            "Epoch 165/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9183 - loss: 0.3502 - learning_rate: 0.0020\n",
            "Epoch 166/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9175 - loss: 0.3456 - learning_rate: 0.0020\n",
            "Epoch 167/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9214 - loss: 0.3375 - learning_rate: 0.0020\n",
            "Epoch 168/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9215 - loss: 0.3304 - learning_rate: 0.0020\n",
            "Epoch 169/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9208 - loss: 0.3265 - learning_rate: 0.0020\n",
            "Epoch 170/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9231 - loss: 0.3256 - learning_rate: 0.0020\n",
            "Epoch 171/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9242 - loss: 0.3228 - learning_rate: 0.0020\n",
            "Epoch 172/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9252 - loss: 0.3155 - learning_rate: 0.0020\n",
            "Epoch 173/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9244 - loss: 0.3154 - learning_rate: 0.0020\n",
            "Epoch 174/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9249 - loss: 0.3096 - learning_rate: 0.0020\n",
            "Epoch 175/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9283 - loss: 0.3010 - learning_rate: 0.0020\n",
            "Epoch 176/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9288 - loss: 0.2980 - learning_rate: 0.0020\n",
            "Epoch 177/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9311 - loss: 0.2914 - learning_rate: 0.0020\n",
            "Epoch 178/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9308 - loss: 0.2894 - learning_rate: 0.0020\n",
            "Epoch 179/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9312 - loss: 0.2855 - learning_rate: 0.0020\n",
            "Epoch 180/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9303 - loss: 0.2879 - learning_rate: 0.0020\n",
            "Epoch 181/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9299 - loss: 0.2897 - learning_rate: 0.0020\n",
            "Epoch 182/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9352 - loss: 0.2754 - learning_rate: 0.0020\n",
            "Epoch 183/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9337 - loss: 0.2759 - learning_rate: 0.0020\n",
            "Epoch 184/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9369 - loss: 0.2695 - learning_rate: 0.0020\n",
            "Epoch 185/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9336 - loss: 0.2728 - learning_rate: 0.0020\n",
            "Epoch 186/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9328 - loss: 0.2743 - learning_rate: 0.0020\n",
            "Epoch 187/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9340 - loss: 0.2686 - learning_rate: 0.0020\n",
            "Epoch 188/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9358 - loss: 0.2668 - learning_rate: 0.0020\n",
            "Epoch 189/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9362 - loss: 0.2632 - learning_rate: 0.0020\n",
            "Epoch 190/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9388 - loss: 0.2561 - learning_rate: 0.0020\n",
            "Epoch 191/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9389 - loss: 0.2546 - learning_rate: 0.0020\n",
            "Epoch 192/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9379 - loss: 0.2548 - learning_rate: 0.0020\n",
            "Epoch 193/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9406 - loss: 0.2488 - learning_rate: 0.0020\n",
            "Epoch 194/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9381 - loss: 0.2514 - learning_rate: 0.0020\n",
            "Epoch 195/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9403 - loss: 0.2475 - learning_rate: 0.0020\n",
            "Epoch 196/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9419 - loss: 0.2380 - learning_rate: 0.0020\n",
            "Epoch 197/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9400 - loss: 0.2435 - learning_rate: 0.0020\n",
            "Epoch 198/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9413 - loss: 0.2454 - learning_rate: 0.0020\n",
            "Epoch 199/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9404 - loss: 0.2411 - learning_rate: 0.0020\n",
            "Epoch 200/200\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9432 - loss: 0.2373 - learning_rate: 0.0020\n",
            "Restoring model weights from the end of the best epoch: 200.\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "EPOCHS = 200\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)\n",
        "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix, save_weights_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='loss', patience=15, restore_best_weights=True, verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqR9fzntNZRi"
      },
      "source": [
        "### 4.2 Learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "SEYV6Eh7wkj_",
        "outputId": "019b4a4f-a640-40b1-ff84-a88f8818b510"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax1.plot(history.history['loss'])\n",
        "ax1.set_title('Loss'); ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss')\n",
        "ax2.plot(history.history['accuracy'])\n",
        "ax2.set_title('Accuracy'); ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEfNE-ke1nGv"
      },
      "source": [
        "## 5 Text Generation (Inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCi-Aq1l1nGv"
      },
      "source": [
        "### 5.1 Sampling Strategy\n",
        "\n",
        "Rather than greedy (argmax) decoding, we use **top-k/top-p nucleus sampling** with temperature:\n",
        "- **Temperature** controls randomness: lower (0.5) = more conservative, higher (1.0) = more creative\n",
        "- **Top-k (k=40)** restricts sampling to the 40 most probable next tokens\n",
        "- **Top-p (p=0.9)** further filters to the smallest set of tokens whose cumulative probability exceeds 0.9\n",
        "- **UNK suppression:** The `<UNK>` token logit is set to −10⁹ before sampling to prevent unknown tokens from appearing in generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeH_EiHP2qxs"
      },
      "outputs": [],
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, model, token_to_id, id_to_token, unk_id=0):\n",
        "        self.model = model\n",
        "        self.token_to_id = token_to_id\n",
        "        self.id_to_token = id_to_token\n",
        "        self.unk_id = unk_id\n",
        "\n",
        "    def encode_prompt(self, text):\n",
        "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
        "        ids = [self.token_to_id.get(t, self.unk_id) for t in tokens]\n",
        "        return ids\n",
        "\n",
        "    def sample_with_top_k_p(self, logits, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        \"\"\"Sample from logits with temperature, top-k, and top-p (nucleus) filtering.\"\"\"\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-k filtering\n",
        "        if top_k > 0:\n",
        "            top_k_vals, top_k_idxs = tf.math.top_k(logits, k=min(top_k, logits.shape[-1]))\n",
        "            # Create mask: set everything outside top-k to -inf\n",
        "            mask = tf.fill(logits.shape, float('-inf'))\n",
        "            mask = tf.tensor_scatter_nd_update(\n",
        "                mask,\n",
        "                tf.expand_dims(top_k_idxs, 1),\n",
        "                top_k_vals\n",
        "            )\n",
        "            logits = mask\n",
        "\n",
        "        # Top-p (nucleus) filtering\n",
        "        if top_p < 1.0:\n",
        "            sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "            sorted_logits = tf.gather(logits, sorted_indices)\n",
        "            cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits), axis=-1)\n",
        "\n",
        "            # Find cutoff index\n",
        "            cutoff_mask = cumulative_probs > top_p\n",
        "            # Shift right so we always keep at least the top token\n",
        "            cutoff_mask = tf.concat([[False], cutoff_mask[:-1]], axis=0)\n",
        "            sorted_logits = tf.where(cutoff_mask, float('-inf'), sorted_logits)\n",
        "\n",
        "            # Unsort\n",
        "            unsort_indices = tf.argsort(sorted_indices)\n",
        "            logits = tf.gather(sorted_logits, unsort_indices)\n",
        "\n",
        "        # Sample\n",
        "        logits_2d = tf.expand_dims(logits, 0)\n",
        "        sampled = tf.random.categorical(logits_2d, num_samples=1)\n",
        "        return int(sampled[0, 0])\n",
        "\n",
        "    def generate(self, prompt, num_tokens=100, temperature=0.8, top_k=40, top_p=0.9):\n",
        "        input_ids = self.encode_prompt(prompt)\n",
        "        if len(input_ids) == 0:\n",
        "            input_ids = [self.unk_id]\n",
        "\n",
        "        generated_ids = list(input_ids)\n",
        "\n",
        "        for _ in range(num_tokens):\n",
        "            context = generated_ids[-SEQ_LENGTH:]\n",
        "            x = tf.expand_dims(context, 0)\n",
        "            predictions = self.model(x, training=False)\n",
        "            logits = predictions[0, -1, :]\n",
        "\n",
        "            # Suppress <UNK> token from generation\n",
        "            logits_np = logits.numpy()\n",
        "            logits_np[self.unk_id] = -1e9\n",
        "            logits = tf.constant(logits_np)\n",
        "\n",
        "            next_id = self.sample_with_top_k_p(logits, temperature, top_k, top_p)\n",
        "            generated_ids.append(next_id)\n",
        "\n",
        "        gen_tokens = [self.id_to_token.get(i, '<UNK>') for i in generated_ids]\n",
        "\n",
        "        result = []\n",
        "        for t in gen_tokens:\n",
        "            if re.match(r'^[^\\w\\s]$', t) and result:\n",
        "                result[-1] += t\n",
        "            else:\n",
        "                result.append(t)\n",
        "        return ' '.join(result)\n",
        "\n",
        "\n",
        "generator = TextGenerator(model, token_to_id, id_to_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8dLkCqy1nGv"
      },
      "source": [
        "### 5.2 Generation Examples\n",
        "Testing with in-domain prompts (Agatha Christie's style) and out-of-domain prompts to evaluate the model's generalisation and failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu42Y00O3NGF",
        "outputId": "1ea7f864-1552-49e9-f990-d468263abf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== IN-DOMAIN PROMPTS ===\n",
            "\n",
            "'Poirot said':\n",
            "Poirot said than nothing. In company with Sir Guy Willard and Dr. Tosswill we were taken round the excavations. The principal finds had been removed to Cairo, but some of the tomb furniture was extremely interesting. The enthusiasm of the world and some of his tomb,\n",
            "\n",
            "'She looked at':\n",
            "She looked at me, but I’ m for my return. Then I knew a question of method. But the maid, it was safely placed in his hands of a most unusual supernatural.” Poirot shook his head.“ We sought, but of course,”\n",
            "\n",
            "=== OUT-OF-DOMAIN PROMPTS ===\n",
            "\n",
            "'The spaceship landed':\n",
            "The <UNK> landed. Rolf had brought us from the same so, and a touch of the greatest quality could have gone out of the window. It was open and Japp were stolen. Of was a low telephone, and he had not been the man who could had fallen\n",
            "\n",
            "'Machine learning is':\n",
            "<UNK> learning is done. He was smiling in his most sign, but I would tell him that morning, as well was in New York, and before night his uncle are rather up from the London and bound up to England. It was found in a car- brush\n"
          ]
        }
      ],
      "source": [
        "# Test with prompts that are familiar with Agatha Christie's ebook and prompts that are not\n",
        "in_book_prompts = [\n",
        "    \"Poirot said\",\n",
        "    \"She looked at\",\n",
        "]\n",
        "\n",
        "out_of_book_prompts = [\n",
        "    \"The spaceship landed\",\n",
        "    \"Machine learning is\",\n",
        "]\n",
        "\n",
        "print(\"=== IN-DOMAIN PROMPTS ===\")\n",
        "for p in in_book_prompts:\n",
        "    print(f\"\\n'{p}':\")\n",
        "    print(generator.generate(p, num_tokens=50, temperature=1.0))\n",
        "\n",
        "print(\"\\n=== OUT-OF-DOMAIN PROMPTS ===\")\n",
        "for p in out_of_book_prompts:\n",
        "    print(f\"\\n'{p}':\")\n",
        "    print(generator.generate(p, num_tokens=50, temperature=1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyazL2fiMXEv"
      },
      "source": [
        "## 6 Comparison: Our Model vs ChatGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv6XKMlbMZfh"
      },
      "source": [
        "1. **'Poirot said'**\n",
        "\n",
        "**Own model**\n",
        "\n",
        "Poirot said than nothing. In company with Sir Guy Willard and Dr. Tosswill we were taken round the excavations. The principal finds had been removed to Cairo, but some of the tomb furniture was extremely interesting. The enthusiasm of the world and some of his tomb,\n",
        "\n",
        "**ChatGPT**\n",
        "\n",
        "Poirot said nothing for a moment, but his eyes, bright as polished jet, travelled slowly around the room. “It is curious, is it not, that we search for footprints upon the carpet, yet overlook the impressions left upon the heart? The murderer, mes amis, has betrayed himself not by what he did, but by what he feared.”\n",
        "\n",
        "2. **'She looked at'**\n",
        "\n",
        "**Own model**\n",
        "\n",
        "She looked at me, but I’ m for my return. Then I knew a question of method. But the maid, it was safely placed in his hands of a most unusual supernatural.” Poirot shook his head.“ We sought, but of course,”\n",
        "\n",
        "**ChatGPT**\n",
        "\n",
        "She looked at the letter in her trembling hand as though it might alter its meaning under the strain of her gaze. The fire crackled cheerfully, in cruel contrast to the chill creeping about her heart. It was not the words themselves that alarmed her, but the careful absence of any signature.\n",
        "\n",
        "3. ### Why the Difference?\n",
        "Our model learns local word co-occurrence statistics from a single book, it predicts the next word based on the preceding 50 words without any semantic understanding. ChatGPT, with ~40,000 more parameters and trained on virtually all published text, has learned language comprehension, narrative structure, and character knowledge. The GRU captures *style* (word choices, sentence rhythms); the LLM captures substance (meaning, plot, character)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1o7IsPeJ875"
      },
      "source": [
        "## 7 Save & Load Model (for later use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsTWTuQiJlbJ",
        "outputId": "0b66135f-161e-4f6f-97c6-33712c37b2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/saved_models/'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model.save(os.path.join(model_save_path, 'language model.keras'))\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "pxDEgeuEJ8kO",
        "outputId": "e183d1be-3cf0-4dc3-990c-640950d41cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from Google Drive!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)          │       \u001b[38;5;34m843,520\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m512\u001b[0m)          │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m3295\u001b[0m)         │     \u001b[38;5;34m1,690,335\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">843,520</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3295</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,690,335</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,877,535\u001b[0m (60.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,877,535</span> (60.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,292,511\u001b[0m (20.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,292,511</span> (20.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m10,585,024\u001b[0m (40.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,585,024</span> (40.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the saved model\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/saved_models/'\n",
        "model_final = tf.keras.models.load_model(\n",
        "    os.path.join(model_save_path, 'language model.keras')\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully from Google Drive!\")\n",
        "\n",
        "# Verify it works\n",
        "model_final.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUOPibn1nGw"
      },
      "source": [
        "## 8 Report\n",
        "\n",
        "**1. Data Processing**\n",
        "\n",
        "The training corpus is a single Agatha Christie ebook (Project Gutenberg #61262, ~68,000 words). Gutenberg header/footer boilerplate was stripped using start/end markers, and whitespace was normalised (collapsing excessive newlines and spaces). Word-level tokenisation was chosen over character-level to improve semantic coherence in generated text. A regex tokeniser `(\\w+|[^\\w\\s])` splits text into words and punctuation tokens. Tokens appearing fewer than 2 times were mapped to a single `<UNK>` token, yielding a vocabulary of 3,295 tokens (4.7% UNK rate). This threshold was selected after comparing MIN_FREQ values of 2-5: lower thresholds preserved more vocabulary but increased model complexity, while higher thresholds (e.g., MIN_FREQ=5, vocab=1,385, which I have experimented while training the model) caused excessive `<UNK>` flooding in generated output despite strong training metrics.\n",
        "\n",
        "**2. Model Architecture & Training**\n",
        "\n",
        "The model is a **2-layer stacked GRU**\n",
        "\n",
        "| Hyperparameter      | Value |\n",
        "|---------------------|-------|\n",
        "| Embedding dimension | 256   |\n",
        "| GRU units (x2 layers)| 512 |\n",
        "| Dropout             | 0.2   |\n",
        "| Sequence length     | 50 words |\n",
        "| Batch size          | 64    |\n",
        "| Optimiser           | Adam (lr=2e-3) |\n",
        "| Epochs              | 150 (with early stopping, patience=15) |\n",
        "\n",
        "Learning rate was reduced on plateau (factor=0.5, patience=5, min_lr=1e-5). The model converged to **loss ≈ 0.4** and **accuracy ≈ 90%** after 150 epochs. At inference, **top-k (k=40) and top-p (p=0.9) nucleus sampling** with temperature scaling is used, and the `<UNK>` token is suppressed (logit set to −10⁹) to prevent unknown tokens in output.\n",
        "\n",
        "**3. Generated Text Examples (temperature=0.8)**\n",
        "\n",
        "**Prompt: \"Poirot said\"**\n",
        "> Poirot said out, rather like a brisk and were sitting out with eleven to close. It was a business- butler round. Lady Yardly was looking out. Lady Yardly were looking out. Lady Yardly were looking round the morning of her chair. She was looking out\n",
        "\n",
        "**Prompt: \"She looked at\"**\n",
        "> She looked at a train home at the train. You were not in the train. By the way, was taken forward, and it was almost part at which you are now.”“ You are not taken! You are not?”“ No, true.\n",
        "\n",
        "**4. Comparison with ChatGPT**\n",
        "\n",
        "Using the same prompts, ChatGPT (GPT-4) produces fluent, plot-coherent prose with accurate character voice, complex sentence structures, and contextually appropriate mystery elements. Our model captures surface-level patterns, like: dialogue formatting, Christie-typical vocabulary (\"Poirot\", \"said\", \"looked\"), and short sentence rhythms; but lacks semantic coherence over longer spans, producing grammatical inconsistencies and repetitive phrasing.\n",
        "\n",
        "This gap stems from fundamental differences: our model (4M parameters) was trained on a single book (68K tokens) using a recurrent architecture that captures local dependencies within a 50-word window. ChatGPT (175B+ parameters) was trained on internet-scale data including Christie's full bibliography, using transformer self-attention that models long-range dependencies across thousands of tokens. Our model performs statistical next-word prediction from local context; ChatGPT has learned language understanding, narrative structure, and character knowledge. The scale difference, which is roughly 40,000x in parameters and millions of times more training data, explains why LLMs produce coherent prose while our model captures style but not substance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 Demo code (testing)"
      ],
      "metadata": {
        "id": "tzrZ70G82uub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/saved_models/'\n",
        "\n",
        "vocab_data = {\n",
        "    'token_to_id': token_to_id,\n",
        "    'id_to_token': {str(k): v for k, v in id_to_token.items()},\n",
        "    'seq_length': SEQ_LENGTH\n",
        "}\n",
        "with open(os.path.join(model_save_path, 'vocab.json'), 'w') as f:\n",
        "    json.dump(vocab_data, f)\n",
        "\n",
        "print(\"Vocabulary saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTT9lji23rY",
        "outputId": "7997b887-4682-4bf3-bfb7-dbe9ca763549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning/saved_models/'\n",
        "\n",
        "# Load model\n",
        "model_final = tf.keras.models.load_model(\n",
        "    os.path.join(model_save_path, 'language model.keras')\n",
        ")\n",
        "\n",
        "# Load vocabulary\n",
        "with open(os.path.join(model_save_path, 'vocab.json'), 'r') as f:\n",
        "    vocab_data = json.load(f)\n",
        "\n",
        "token_to_id = vocab_data['token_to_id']\n",
        "id_to_token = {int(k): v for k, v in vocab_data['id_to_token'].items()}\n",
        "SEQ_LENGTH = vocab_data['seq_length']\n",
        "\n",
        "# Create generator (requires TextGenerator class cell to be run first)\n",
        "generator = TextGenerator(model_final, token_to_id, id_to_token)\n",
        "print(\"Ready for generation!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GN7bgYM3EmE",
        "outputId": "5d0b24b9-1688-4503-ee44-830b3aa709f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready for generation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 55)\n",
        "print(\"  Agatha Christie Text Generator Demo\")\n",
        "print(\"  Enter any text (word, sentence, or paragraph).\")\n",
        "print(\"  The model will generate at least 20 words.\")\n",
        "print(\"  Type 'q' to exit.\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter text prompt: \").strip()\n",
        "    if user_input.lower() == 'q':\n",
        "        break\n",
        "    if not user_input:\n",
        "        print(\"Please enter some text.\")\n",
        "        continue\n",
        "\n",
        "    # Count input words for display\n",
        "    input_word_count = len(user_input.split())\n",
        "\n",
        "    # Generate at least 20 NEW words beyond the prompt\n",
        "    output = generator.generate(user_input, num_tokens=50, temperature=0.8)\n",
        "\n",
        "    # Count generated words (beyond prompt)\n",
        "    output_words = output.split()\n",
        "    generated_word_count = len(output_words) - input_word_count\n",
        "\n",
        "    print(f\"\\n--- Input ({input_word_count} words) ---\")\n",
        "    print(user_input)\n",
        "    print(f\"\\n--- Generated output ({generated_word_count} new words) ---\")\n",
        "    print(output)\n",
        "    print(\"-\" * 55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jTsJP-13MVD",
        "outputId": "e2e84ee2-ad75-468f-881e-b873b1522672"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=======================================================\n",
            "  Agatha Christie Text Generator Demo\n",
            "  Enter any text (word, sentence, or paragraph).\n",
            "  The model will generate at least 20 words.\n",
            "  Type 'q' to exit.\n",
            "=======================================================\n",
            "\n",
            "Enter text prompt: q\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}